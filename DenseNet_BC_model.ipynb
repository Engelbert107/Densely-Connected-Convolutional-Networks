{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNet-BC-model-Engelbert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W1C3XZqU84d"
      },
      "source": [
        "# DenseNet-BC implementation with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG-arV36yJb8"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import random\n",
        "import torchsummary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeYvMzm-yTWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19825254-89f8-4850-c5b8-02260e41e2ae"
      },
      "source": [
        "print(torch.__version__)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9.0+cu102\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9OZd9XaykqQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d13fde94-c514-428b-d5f0-7485e57e14b1"
      },
      "source": [
        "validation_ratio = 0.1\n",
        "random_seed = 10\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
        "\n",
        "transform_validation = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
        "\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "\n",
        "validset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_validation)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "\n",
        "num_train = len(trainset)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(validation_ratio * num_train))\n",
        "\n",
        "np.random.seed(random_seed)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# Data Loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=batch_size, sampler=train_sampler, num_workers=0\n",
        ")\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    validset, batch_size=batch_size, sampler=valid_sampler, num_workers=0\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=batch_size, shuffle=False, num_workers=0\n",
        ")\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-r-IAS106N0"
      },
      "source": [
        "class bn_relu_conv(nn.Module):\n",
        "    def __init__(self, nin, nout, kernel_size, stride, padding, bias=False):\n",
        "        super(bn_relu_conv, self).__init__()\n",
        "        self.batch_norm = nn.BatchNorm2d(nin)\n",
        "        self.relu = nn.ReLU(True)\n",
        "        self.conv = nn.Conv2d(nin, nout, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.batch_norm(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOJHDgZX3TxJ"
      },
      "source": [
        "class bottleneck_layer(nn.Sequential):\n",
        "  def __init__(self, nin, growth_rate, drop_rate=0.2):    \n",
        "      super(bottleneck_layer, self).__init__()\n",
        "      \n",
        "      self.add_module('conv_1x1', bn_relu_conv(nin=nin, nout=growth_rate*4, kernel_size=1, stride=1, padding=0, bias=False))\n",
        "      self.add_module('conv_3x3', bn_relu_conv(nin=growth_rate*4, nout=growth_rate, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "      \n",
        "      self.drop_rate = drop_rate\n",
        "      \n",
        "  def forward(self, x):\n",
        "      bottleneck_output = super(bottleneck_layer, self).forward(x)\n",
        "      if self.drop_rate > 0:\n",
        "          bottleneck_output = F.dropout(bottleneck_output, p=self.drop_rate, training=self.training)\n",
        "          \n",
        "      bottleneck_output = torch.cat((x, bottleneck_output), 1)\n",
        "      \n",
        "      return bottleneck_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qszCjalp4qNX"
      },
      "source": [
        "class Transition_layer(nn.Sequential):\n",
        "  def __init__(self, nin, theta=0.5):    \n",
        "      super(Transition_layer, self).__init__()\n",
        "      \n",
        "      self.add_module('conv_1x1', bn_relu_conv(nin=nin, nout=int(nin*theta), kernel_size=1, stride=1, padding=0, bias=False))\n",
        "      self.add_module('avg_pool_2x2', nn.AvgPool2d(kernel_size=2, stride=2, padding=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S00fPblY4-ha"
      },
      "source": [
        "class DenseBlock(nn.Sequential):\n",
        "  def __init__(self, nin, num_bottleneck_layers, growth_rate, drop_rate=0.2):\n",
        "      super(DenseBlock, self).__init__()\n",
        "                        \n",
        "      for i in range(num_bottleneck_layers):\n",
        "          nin_bottleneck_layer = nin + growth_rate * i\n",
        "          self.add_module('bottleneck_layer_%d' % i, bottleneck_layer(nin=nin_bottleneck_layer, growth_rate=growth_rate, drop_rate=drop_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcqvpn2ly3lL"
      },
      "source": [
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, growth_rate=12, num_layers=100, theta=0.5, drop_rate=0.2, num_classes=10):\n",
        "        super(DenseNet, self).__init__()\n",
        "        \n",
        "        assert (num_layers - 4) % 6 == 0\n",
        "        \n",
        "        # (num_layers-4)//6 \n",
        "        num_bottleneck_layers = (num_layers - 4) // 6\n",
        "        \n",
        "        # 32 x 32 x 3 --> 32 x 32 x (growth_rate*2)\n",
        "        self.dense_init = nn.Conv2d(3, growth_rate*2, kernel_size=3, stride=1, padding=1, bias=True)\n",
        "                \n",
        "        # 32 x 32 x (growth_rate*2) --> 32 x 32 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)]\n",
        "        self.dense_block_1 = DenseBlock(nin=growth_rate*2, num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "\n",
        "        # 32 x 32 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)] --> 16 x 16 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)]*theta\n",
        "        nin_transition_layer_1 = (growth_rate*2) + (growth_rate * num_bottleneck_layers) \n",
        "        self.transition_layer_1 = Transition_layer(nin=nin_transition_layer_1, theta=theta)\n",
        "        \n",
        "        # 16 x 16 x nin_transition_layer_1*theta --> 16 x 16 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)]\n",
        "        self.dense_block_2 = DenseBlock(nin=int(nin_transition_layer_1*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "\n",
        "        # 16 x 16 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)] --> 8 x 8 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)]*theta\n",
        "        nin_transition_layer_2 = int(nin_transition_layer_1*theta) + (growth_rate * num_bottleneck_layers) \n",
        "        self.transition_layer_2 = Transition_layer(nin=nin_transition_layer_2, theta=theta)\n",
        "        \n",
        "        # 8 x 8 x nin_transition_layer_2*theta --> 8 x 8 x [nin_transition_layer_2*theta + (growth_rate * num_bottleneck_layers)]\n",
        "        self.dense_block_3 = DenseBlock(nin=int(nin_transition_layer_2*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "        \n",
        "        nin_fc_layer = int(nin_transition_layer_2*theta) + (growth_rate * num_bottleneck_layers) \n",
        "        \n",
        "        # [nin_transition_layer_2*theta + (growth_rate * num_bottleneck_layers)] --> num_classes\n",
        "        self.fc_layer = nn.Linear(nin_fc_layer, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        dense_init_output = self.dense_init(x)\n",
        "        \n",
        "        dense_block_1_output = self.dense_block_1(dense_init_output)\n",
        "        transition_layer_1_output = self.transition_layer_1(dense_block_1_output)\n",
        "        \n",
        "        dense_block_2_output = self.dense_block_2(transition_layer_1_output)\n",
        "        transition_layer_2_output = self.transition_layer_2(dense_block_2_output)\n",
        "        \n",
        "        dense_block_3_output = self.dense_block_3(transition_layer_2_output)\n",
        "        \n",
        "        global_avg_pool_output = F.adaptive_avg_pool2d(dense_block_3_output, (1, 1))                \n",
        "        global_avg_pool_output_flat = global_avg_pool_output.view(global_avg_pool_output.size(0), -1)\n",
        "\n",
        "        output = self.fc_layer(global_avg_pool_output_flat)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_Pu_kXLPKd_"
      },
      "source": [
        "def DenseNetBC_100_12():\n",
        "    return DenseNet(growth_rate=12, num_layers=100, theta=0.5, drop_rate=0.2, num_classes=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJB5NVcYPnB9"
      },
      "source": [
        "densenetBC = DenseNetBC_100_12()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnWt7hXVSIxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bb0d8f5-a4c9-46f5-93e7-c1818b20aa31"
      },
      "source": [
        "densenetBC.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseNet(\n",
              "  (dense_init): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (dense_block_1): DenseBlock(\n",
              "    (bottleneck_layer_0): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_1): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_2): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_3): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_4): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_5): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_6): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_7): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_8): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_9): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_10): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_11): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_12): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_13): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_14): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_15): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (transition_layer_1): Transition_layer(\n",
              "    (conv_1x1): bn_relu_conv(\n",
              "      (batch_norm): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv): Conv2d(216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (avg_pool_2x2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "  )\n",
              "  (dense_block_2): DenseBlock(\n",
              "    (bottleneck_layer_0): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_1): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_2): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_3): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_4): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_5): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_6): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_7): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_8): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_9): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_10): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_11): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_12): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_13): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_14): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_15): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (transition_layer_2): Transition_layer(\n",
              "    (conv_1x1): bn_relu_conv(\n",
              "      (batch_norm): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv): Conv2d(300, 150, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (avg_pool_2x2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "  )\n",
              "  (dense_block_3): DenseBlock(\n",
              "    (bottleneck_layer_0): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_1): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_2): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(174, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_3): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(186, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_4): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(198, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_5): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(210, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_6): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(222, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_7): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(234, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_8): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(246, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_9): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(258, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_10): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(270, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_11): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(282, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(282, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_12): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(294, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_13): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(306, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_14): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(318, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_15): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(330, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc_layer): Linear(in_features=342, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA5Y5Wpfgv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59ee1ce3-bd97-4519-a198-11b4a0e8786b"
      },
      "source": [
        "torchsummary.summary(densenetBC, (3, 32, 32))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 24, 32, 32]             672\n",
            "       BatchNorm2d-2           [-1, 24, 32, 32]              48\n",
            "              ReLU-3           [-1, 24, 32, 32]               0\n",
            "            Conv2d-4           [-1, 48, 32, 32]           1,152\n",
            "      bn_relu_conv-5           [-1, 48, 32, 32]               0\n",
            "       BatchNorm2d-6           [-1, 48, 32, 32]              96\n",
            "              ReLU-7           [-1, 48, 32, 32]               0\n",
            "            Conv2d-8           [-1, 12, 32, 32]           5,184\n",
            "      bn_relu_conv-9           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-10           [-1, 36, 32, 32]              72\n",
            "             ReLU-11           [-1, 36, 32, 32]               0\n",
            "           Conv2d-12           [-1, 48, 32, 32]           1,728\n",
            "     bn_relu_conv-13           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-14           [-1, 48, 32, 32]              96\n",
            "             ReLU-15           [-1, 48, 32, 32]               0\n",
            "           Conv2d-16           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-17           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-18           [-1, 48, 32, 32]              96\n",
            "             ReLU-19           [-1, 48, 32, 32]               0\n",
            "           Conv2d-20           [-1, 48, 32, 32]           2,304\n",
            "     bn_relu_conv-21           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-22           [-1, 48, 32, 32]              96\n",
            "             ReLU-23           [-1, 48, 32, 32]               0\n",
            "           Conv2d-24           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-25           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-26           [-1, 60, 32, 32]             120\n",
            "             ReLU-27           [-1, 60, 32, 32]               0\n",
            "           Conv2d-28           [-1, 48, 32, 32]           2,880\n",
            "     bn_relu_conv-29           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-30           [-1, 48, 32, 32]              96\n",
            "             ReLU-31           [-1, 48, 32, 32]               0\n",
            "           Conv2d-32           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-33           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-34           [-1, 72, 32, 32]             144\n",
            "             ReLU-35           [-1, 72, 32, 32]               0\n",
            "           Conv2d-36           [-1, 48, 32, 32]           3,456\n",
            "     bn_relu_conv-37           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-38           [-1, 48, 32, 32]              96\n",
            "             ReLU-39           [-1, 48, 32, 32]               0\n",
            "           Conv2d-40           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-41           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-42           [-1, 84, 32, 32]             168\n",
            "             ReLU-43           [-1, 84, 32, 32]               0\n",
            "           Conv2d-44           [-1, 48, 32, 32]           4,032\n",
            "     bn_relu_conv-45           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-46           [-1, 48, 32, 32]              96\n",
            "             ReLU-47           [-1, 48, 32, 32]               0\n",
            "           Conv2d-48           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-49           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-50           [-1, 96, 32, 32]             192\n",
            "             ReLU-51           [-1, 96, 32, 32]               0\n",
            "           Conv2d-52           [-1, 48, 32, 32]           4,608\n",
            "     bn_relu_conv-53           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-54           [-1, 48, 32, 32]              96\n",
            "             ReLU-55           [-1, 48, 32, 32]               0\n",
            "           Conv2d-56           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-57           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-58          [-1, 108, 32, 32]             216\n",
            "             ReLU-59          [-1, 108, 32, 32]               0\n",
            "           Conv2d-60           [-1, 48, 32, 32]           5,184\n",
            "     bn_relu_conv-61           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-62           [-1, 48, 32, 32]              96\n",
            "             ReLU-63           [-1, 48, 32, 32]               0\n",
            "           Conv2d-64           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-65           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-66          [-1, 120, 32, 32]             240\n",
            "             ReLU-67          [-1, 120, 32, 32]               0\n",
            "           Conv2d-68           [-1, 48, 32, 32]           5,760\n",
            "     bn_relu_conv-69           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-70           [-1, 48, 32, 32]              96\n",
            "             ReLU-71           [-1, 48, 32, 32]               0\n",
            "           Conv2d-72           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-73           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-74          [-1, 132, 32, 32]             264\n",
            "             ReLU-75          [-1, 132, 32, 32]               0\n",
            "           Conv2d-76           [-1, 48, 32, 32]           6,336\n",
            "     bn_relu_conv-77           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-78           [-1, 48, 32, 32]              96\n",
            "             ReLU-79           [-1, 48, 32, 32]               0\n",
            "           Conv2d-80           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-81           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-82          [-1, 144, 32, 32]             288\n",
            "             ReLU-83          [-1, 144, 32, 32]               0\n",
            "           Conv2d-84           [-1, 48, 32, 32]           6,912\n",
            "     bn_relu_conv-85           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-86           [-1, 48, 32, 32]              96\n",
            "             ReLU-87           [-1, 48, 32, 32]               0\n",
            "           Conv2d-88           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-89           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-90          [-1, 156, 32, 32]             312\n",
            "             ReLU-91          [-1, 156, 32, 32]               0\n",
            "           Conv2d-92           [-1, 48, 32, 32]           7,488\n",
            "     bn_relu_conv-93           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-94           [-1, 48, 32, 32]              96\n",
            "             ReLU-95           [-1, 48, 32, 32]               0\n",
            "           Conv2d-96           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-97           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-98          [-1, 168, 32, 32]             336\n",
            "             ReLU-99          [-1, 168, 32, 32]               0\n",
            "          Conv2d-100           [-1, 48, 32, 32]           8,064\n",
            "    bn_relu_conv-101           [-1, 48, 32, 32]               0\n",
            "     BatchNorm2d-102           [-1, 48, 32, 32]              96\n",
            "            ReLU-103           [-1, 48, 32, 32]               0\n",
            "          Conv2d-104           [-1, 12, 32, 32]           5,184\n",
            "    bn_relu_conv-105           [-1, 12, 32, 32]               0\n",
            "     BatchNorm2d-106          [-1, 180, 32, 32]             360\n",
            "            ReLU-107          [-1, 180, 32, 32]               0\n",
            "          Conv2d-108           [-1, 48, 32, 32]           8,640\n",
            "    bn_relu_conv-109           [-1, 48, 32, 32]               0\n",
            "     BatchNorm2d-110           [-1, 48, 32, 32]              96\n",
            "            ReLU-111           [-1, 48, 32, 32]               0\n",
            "          Conv2d-112           [-1, 12, 32, 32]           5,184\n",
            "    bn_relu_conv-113           [-1, 12, 32, 32]               0\n",
            "     BatchNorm2d-114          [-1, 192, 32, 32]             384\n",
            "            ReLU-115          [-1, 192, 32, 32]               0\n",
            "          Conv2d-116           [-1, 48, 32, 32]           9,216\n",
            "    bn_relu_conv-117           [-1, 48, 32, 32]               0\n",
            "     BatchNorm2d-118           [-1, 48, 32, 32]              96\n",
            "            ReLU-119           [-1, 48, 32, 32]               0\n",
            "          Conv2d-120           [-1, 12, 32, 32]           5,184\n",
            "    bn_relu_conv-121           [-1, 12, 32, 32]               0\n",
            "     BatchNorm2d-122          [-1, 204, 32, 32]             408\n",
            "            ReLU-123          [-1, 204, 32, 32]               0\n",
            "          Conv2d-124           [-1, 48, 32, 32]           9,792\n",
            "    bn_relu_conv-125           [-1, 48, 32, 32]               0\n",
            "     BatchNorm2d-126           [-1, 48, 32, 32]              96\n",
            "            ReLU-127           [-1, 48, 32, 32]               0\n",
            "          Conv2d-128           [-1, 12, 32, 32]           5,184\n",
            "    bn_relu_conv-129           [-1, 12, 32, 32]               0\n",
            "     BatchNorm2d-130          [-1, 216, 32, 32]             432\n",
            "            ReLU-131          [-1, 216, 32, 32]               0\n",
            "          Conv2d-132          [-1, 108, 32, 32]          23,328\n",
            "    bn_relu_conv-133          [-1, 108, 32, 32]               0\n",
            "       AvgPool2d-134          [-1, 108, 16, 16]               0\n",
            "     BatchNorm2d-135          [-1, 108, 16, 16]             216\n",
            "            ReLU-136          [-1, 108, 16, 16]               0\n",
            "          Conv2d-137           [-1, 48, 16, 16]           5,184\n",
            "    bn_relu_conv-138           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-139           [-1, 48, 16, 16]              96\n",
            "            ReLU-140           [-1, 48, 16, 16]               0\n",
            "          Conv2d-141           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-142           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-143          [-1, 120, 16, 16]             240\n",
            "            ReLU-144          [-1, 120, 16, 16]               0\n",
            "          Conv2d-145           [-1, 48, 16, 16]           5,760\n",
            "    bn_relu_conv-146           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-147           [-1, 48, 16, 16]              96\n",
            "            ReLU-148           [-1, 48, 16, 16]               0\n",
            "          Conv2d-149           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-150           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-151          [-1, 132, 16, 16]             264\n",
            "            ReLU-152          [-1, 132, 16, 16]               0\n",
            "          Conv2d-153           [-1, 48, 16, 16]           6,336\n",
            "    bn_relu_conv-154           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-155           [-1, 48, 16, 16]              96\n",
            "            ReLU-156           [-1, 48, 16, 16]               0\n",
            "          Conv2d-157           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-158           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-159          [-1, 144, 16, 16]             288\n",
            "            ReLU-160          [-1, 144, 16, 16]               0\n",
            "          Conv2d-161           [-1, 48, 16, 16]           6,912\n",
            "    bn_relu_conv-162           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-163           [-1, 48, 16, 16]              96\n",
            "            ReLU-164           [-1, 48, 16, 16]               0\n",
            "          Conv2d-165           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-166           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-167          [-1, 156, 16, 16]             312\n",
            "            ReLU-168          [-1, 156, 16, 16]               0\n",
            "          Conv2d-169           [-1, 48, 16, 16]           7,488\n",
            "    bn_relu_conv-170           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-171           [-1, 48, 16, 16]              96\n",
            "            ReLU-172           [-1, 48, 16, 16]               0\n",
            "          Conv2d-173           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-174           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-175          [-1, 168, 16, 16]             336\n",
            "            ReLU-176          [-1, 168, 16, 16]               0\n",
            "          Conv2d-177           [-1, 48, 16, 16]           8,064\n",
            "    bn_relu_conv-178           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-179           [-1, 48, 16, 16]              96\n",
            "            ReLU-180           [-1, 48, 16, 16]               0\n",
            "          Conv2d-181           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-182           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-183          [-1, 180, 16, 16]             360\n",
            "            ReLU-184          [-1, 180, 16, 16]               0\n",
            "          Conv2d-185           [-1, 48, 16, 16]           8,640\n",
            "    bn_relu_conv-186           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-187           [-1, 48, 16, 16]              96\n",
            "            ReLU-188           [-1, 48, 16, 16]               0\n",
            "          Conv2d-189           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-190           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-191          [-1, 192, 16, 16]             384\n",
            "            ReLU-192          [-1, 192, 16, 16]               0\n",
            "          Conv2d-193           [-1, 48, 16, 16]           9,216\n",
            "    bn_relu_conv-194           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-195           [-1, 48, 16, 16]              96\n",
            "            ReLU-196           [-1, 48, 16, 16]               0\n",
            "          Conv2d-197           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-198           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-199          [-1, 204, 16, 16]             408\n",
            "            ReLU-200          [-1, 204, 16, 16]               0\n",
            "          Conv2d-201           [-1, 48, 16, 16]           9,792\n",
            "    bn_relu_conv-202           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-203           [-1, 48, 16, 16]              96\n",
            "            ReLU-204           [-1, 48, 16, 16]               0\n",
            "          Conv2d-205           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-206           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-207          [-1, 216, 16, 16]             432\n",
            "            ReLU-208          [-1, 216, 16, 16]               0\n",
            "          Conv2d-209           [-1, 48, 16, 16]          10,368\n",
            "    bn_relu_conv-210           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-211           [-1, 48, 16, 16]              96\n",
            "            ReLU-212           [-1, 48, 16, 16]               0\n",
            "          Conv2d-213           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-214           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-215          [-1, 228, 16, 16]             456\n",
            "            ReLU-216          [-1, 228, 16, 16]               0\n",
            "          Conv2d-217           [-1, 48, 16, 16]          10,944\n",
            "    bn_relu_conv-218           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-219           [-1, 48, 16, 16]              96\n",
            "            ReLU-220           [-1, 48, 16, 16]               0\n",
            "          Conv2d-221           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-222           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-223          [-1, 240, 16, 16]             480\n",
            "            ReLU-224          [-1, 240, 16, 16]               0\n",
            "          Conv2d-225           [-1, 48, 16, 16]          11,520\n",
            "    bn_relu_conv-226           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-227           [-1, 48, 16, 16]              96\n",
            "            ReLU-228           [-1, 48, 16, 16]               0\n",
            "          Conv2d-229           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-230           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-231          [-1, 252, 16, 16]             504\n",
            "            ReLU-232          [-1, 252, 16, 16]               0\n",
            "          Conv2d-233           [-1, 48, 16, 16]          12,096\n",
            "    bn_relu_conv-234           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-235           [-1, 48, 16, 16]              96\n",
            "            ReLU-236           [-1, 48, 16, 16]               0\n",
            "          Conv2d-237           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-238           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-239          [-1, 264, 16, 16]             528\n",
            "            ReLU-240          [-1, 264, 16, 16]               0\n",
            "          Conv2d-241           [-1, 48, 16, 16]          12,672\n",
            "    bn_relu_conv-242           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-243           [-1, 48, 16, 16]              96\n",
            "            ReLU-244           [-1, 48, 16, 16]               0\n",
            "          Conv2d-245           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-246           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-247          [-1, 276, 16, 16]             552\n",
            "            ReLU-248          [-1, 276, 16, 16]               0\n",
            "          Conv2d-249           [-1, 48, 16, 16]          13,248\n",
            "    bn_relu_conv-250           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-251           [-1, 48, 16, 16]              96\n",
            "            ReLU-252           [-1, 48, 16, 16]               0\n",
            "          Conv2d-253           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-254           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-255          [-1, 288, 16, 16]             576\n",
            "            ReLU-256          [-1, 288, 16, 16]               0\n",
            "          Conv2d-257           [-1, 48, 16, 16]          13,824\n",
            "    bn_relu_conv-258           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-259           [-1, 48, 16, 16]              96\n",
            "            ReLU-260           [-1, 48, 16, 16]               0\n",
            "          Conv2d-261           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-262           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-263          [-1, 300, 16, 16]             600\n",
            "            ReLU-264          [-1, 300, 16, 16]               0\n",
            "          Conv2d-265          [-1, 150, 16, 16]          45,000\n",
            "    bn_relu_conv-266          [-1, 150, 16, 16]               0\n",
            "       AvgPool2d-267            [-1, 150, 8, 8]               0\n",
            "     BatchNorm2d-268            [-1, 150, 8, 8]             300\n",
            "            ReLU-269            [-1, 150, 8, 8]               0\n",
            "          Conv2d-270             [-1, 48, 8, 8]           7,200\n",
            "    bn_relu_conv-271             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-272             [-1, 48, 8, 8]              96\n",
            "            ReLU-273             [-1, 48, 8, 8]               0\n",
            "          Conv2d-274             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-275             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-276            [-1, 162, 8, 8]             324\n",
            "            ReLU-277            [-1, 162, 8, 8]               0\n",
            "          Conv2d-278             [-1, 48, 8, 8]           7,776\n",
            "    bn_relu_conv-279             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-280             [-1, 48, 8, 8]              96\n",
            "            ReLU-281             [-1, 48, 8, 8]               0\n",
            "          Conv2d-282             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-283             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-284            [-1, 174, 8, 8]             348\n",
            "            ReLU-285            [-1, 174, 8, 8]               0\n",
            "          Conv2d-286             [-1, 48, 8, 8]           8,352\n",
            "    bn_relu_conv-287             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-288             [-1, 48, 8, 8]              96\n",
            "            ReLU-289             [-1, 48, 8, 8]               0\n",
            "          Conv2d-290             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-291             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-292            [-1, 186, 8, 8]             372\n",
            "            ReLU-293            [-1, 186, 8, 8]               0\n",
            "          Conv2d-294             [-1, 48, 8, 8]           8,928\n",
            "    bn_relu_conv-295             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-296             [-1, 48, 8, 8]              96\n",
            "            ReLU-297             [-1, 48, 8, 8]               0\n",
            "          Conv2d-298             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-299             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-300            [-1, 198, 8, 8]             396\n",
            "            ReLU-301            [-1, 198, 8, 8]               0\n",
            "          Conv2d-302             [-1, 48, 8, 8]           9,504\n",
            "    bn_relu_conv-303             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-304             [-1, 48, 8, 8]              96\n",
            "            ReLU-305             [-1, 48, 8, 8]               0\n",
            "          Conv2d-306             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-307             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-308            [-1, 210, 8, 8]             420\n",
            "            ReLU-309            [-1, 210, 8, 8]               0\n",
            "          Conv2d-310             [-1, 48, 8, 8]          10,080\n",
            "    bn_relu_conv-311             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-312             [-1, 48, 8, 8]              96\n",
            "            ReLU-313             [-1, 48, 8, 8]               0\n",
            "          Conv2d-314             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-315             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-316            [-1, 222, 8, 8]             444\n",
            "            ReLU-317            [-1, 222, 8, 8]               0\n",
            "          Conv2d-318             [-1, 48, 8, 8]          10,656\n",
            "    bn_relu_conv-319             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-320             [-1, 48, 8, 8]              96\n",
            "            ReLU-321             [-1, 48, 8, 8]               0\n",
            "          Conv2d-322             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-323             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-324            [-1, 234, 8, 8]             468\n",
            "            ReLU-325            [-1, 234, 8, 8]               0\n",
            "          Conv2d-326             [-1, 48, 8, 8]          11,232\n",
            "    bn_relu_conv-327             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-328             [-1, 48, 8, 8]              96\n",
            "            ReLU-329             [-1, 48, 8, 8]               0\n",
            "          Conv2d-330             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-331             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-332            [-1, 246, 8, 8]             492\n",
            "            ReLU-333            [-1, 246, 8, 8]               0\n",
            "          Conv2d-334             [-1, 48, 8, 8]          11,808\n",
            "    bn_relu_conv-335             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-336             [-1, 48, 8, 8]              96\n",
            "            ReLU-337             [-1, 48, 8, 8]               0\n",
            "          Conv2d-338             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-339             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-340            [-1, 258, 8, 8]             516\n",
            "            ReLU-341            [-1, 258, 8, 8]               0\n",
            "          Conv2d-342             [-1, 48, 8, 8]          12,384\n",
            "    bn_relu_conv-343             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-344             [-1, 48, 8, 8]              96\n",
            "            ReLU-345             [-1, 48, 8, 8]               0\n",
            "          Conv2d-346             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-347             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-348            [-1, 270, 8, 8]             540\n",
            "            ReLU-349            [-1, 270, 8, 8]               0\n",
            "          Conv2d-350             [-1, 48, 8, 8]          12,960\n",
            "    bn_relu_conv-351             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-352             [-1, 48, 8, 8]              96\n",
            "            ReLU-353             [-1, 48, 8, 8]               0\n",
            "          Conv2d-354             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-355             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-356            [-1, 282, 8, 8]             564\n",
            "            ReLU-357            [-1, 282, 8, 8]               0\n",
            "          Conv2d-358             [-1, 48, 8, 8]          13,536\n",
            "    bn_relu_conv-359             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-360             [-1, 48, 8, 8]              96\n",
            "            ReLU-361             [-1, 48, 8, 8]               0\n",
            "          Conv2d-362             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-363             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-364            [-1, 294, 8, 8]             588\n",
            "            ReLU-365            [-1, 294, 8, 8]               0\n",
            "          Conv2d-366             [-1, 48, 8, 8]          14,112\n",
            "    bn_relu_conv-367             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-368             [-1, 48, 8, 8]              96\n",
            "            ReLU-369             [-1, 48, 8, 8]               0\n",
            "          Conv2d-370             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-371             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-372            [-1, 306, 8, 8]             612\n",
            "            ReLU-373            [-1, 306, 8, 8]               0\n",
            "          Conv2d-374             [-1, 48, 8, 8]          14,688\n",
            "    bn_relu_conv-375             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-376             [-1, 48, 8, 8]              96\n",
            "            ReLU-377             [-1, 48, 8, 8]               0\n",
            "          Conv2d-378             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-379             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-380            [-1, 318, 8, 8]             636\n",
            "            ReLU-381            [-1, 318, 8, 8]               0\n",
            "          Conv2d-382             [-1, 48, 8, 8]          15,264\n",
            "    bn_relu_conv-383             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-384             [-1, 48, 8, 8]              96\n",
            "            ReLU-385             [-1, 48, 8, 8]               0\n",
            "          Conv2d-386             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-387             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-388            [-1, 330, 8, 8]             660\n",
            "            ReLU-389            [-1, 330, 8, 8]               0\n",
            "          Conv2d-390             [-1, 48, 8, 8]          15,840\n",
            "    bn_relu_conv-391             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-392             [-1, 48, 8, 8]              96\n",
            "            ReLU-393             [-1, 48, 8, 8]               0\n",
            "          Conv2d-394             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-395             [-1, 12, 8, 8]               0\n",
            "          Linear-396                   [-1, 10]           3,430\n",
            "================================================================\n",
            "Total params: 768,502\n",
            "Trainable params: 768,502\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 87.35\n",
            "Params size (MB): 2.93\n",
            "Estimated Total Size (MB): 90.30\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtjYUJbQPvPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "792d926a-b80d-409e-9155-36569e7c3348"
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "learning_rate = 0.1\n",
        "num_epoch = 40\n",
        "loss_vector = []\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Oprimization Criteria and Optimization method\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(densenetBC.parameters(), lr=learning_rate, momentum=0.9)\n",
        "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[int(num_epoch * 0.5), int(num_epoch * 0.75)], gamma=0.1, last_epoch=-1)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epoch):  \n",
        "    lr_scheduler.step()\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = densenetBC(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        show_period = 100\n",
        "        if i % show_period == show_period-1:    # print every \"show_period\" mini-batches\n",
        "            loss_vector.append(running_loss / show_period)\n",
        "            print('[%d, %5d/50000] loss: %.7f' %\n",
        "                  (epoch + 1, (i + 1)*batch_size, running_loss / show_period))\n",
        "            running_loss = 0.0\n",
        "        \n",
        "        \n",
        "    # validation part\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(valid_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = densenetBC(inputs)\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "    print('[%d epoch] Accuracy of the network on the validation images: %d %%' % \n",
        "          (epoch + 1, 100 * correct / total)\n",
        "         )\n",
        "\n",
        "print('Finished Training')\n",
        "end = time.time() # Time counted in seconds\n",
        "print(f\"The total time to train the model on the K80 GPU is : {((end - start)/60):.1f} minutes.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,  6400/50000] loss: 1.9896438\n",
            "[1, 12800/50000] loss: 1.8472652\n",
            "[1, 19200/50000] loss: 1.7422154\n",
            "[1, 25600/50000] loss: 1.6270522\n",
            "[1, 32000/50000] loss: 1.5546020\n",
            "[1, 38400/50000] loss: 1.4678522\n",
            "[1, 44800/50000] loss: 1.4017360\n",
            "[1 epoch] Accuracy of the network on the validation images: 42 %\n",
            "[2,  6400/50000] loss: 1.3521483\n",
            "[2, 12800/50000] loss: 1.2921677\n",
            "[2, 19200/50000] loss: 1.2528612\n",
            "[2, 25600/50000] loss: 1.1459972\n",
            "[2, 32000/50000] loss: 1.1028980\n",
            "[2, 38400/50000] loss: 1.0719605\n",
            "[2, 44800/50000] loss: 1.0964580\n",
            "[2 epoch] Accuracy of the network on the validation images: 55 %\n",
            "[3,  6400/50000] loss: 1.0502053\n",
            "[3, 12800/50000] loss: 0.9985137\n",
            "[3, 19200/50000] loss: 0.9693762\n",
            "[3, 25600/50000] loss: 0.9674620\n",
            "[3, 32000/50000] loss: 0.8693732\n",
            "[3, 38400/50000] loss: 0.8971371\n",
            "[3, 44800/50000] loss: 0.9046278\n",
            "[3 epoch] Accuracy of the network on the validation images: 65 %\n",
            "[4,  6400/50000] loss: 0.8780809\n",
            "[4, 12800/50000] loss: 0.8545940\n",
            "[4, 19200/50000] loss: 0.8556324\n",
            "[4, 25600/50000] loss: 0.7780161\n",
            "[4, 32000/50000] loss: 0.7971866\n",
            "[4, 38400/50000] loss: 0.7591564\n",
            "[4, 44800/50000] loss: 0.7330045\n",
            "[4 epoch] Accuracy of the network on the validation images: 66 %\n",
            "[5,  6400/50000] loss: 0.7515835\n",
            "[5, 12800/50000] loss: 0.7076418\n",
            "[5, 19200/50000] loss: 0.7071317\n",
            "[5, 25600/50000] loss: 0.6994010\n",
            "[5, 32000/50000] loss: 0.6765868\n",
            "[5, 38400/50000] loss: 0.6906431\n",
            "[5, 44800/50000] loss: 0.6774377\n",
            "[5 epoch] Accuracy of the network on the validation images: 71 %\n",
            "[6,  6400/50000] loss: 0.6672928\n",
            "[6, 12800/50000] loss: 0.6543113\n",
            "[6, 19200/50000] loss: 0.6281163\n",
            "[6, 25600/50000] loss: 0.6404517\n",
            "[6, 32000/50000] loss: 0.6142702\n",
            "[6, 38400/50000] loss: 0.6147844\n",
            "[6, 44800/50000] loss: 0.5916319\n",
            "[6 epoch] Accuracy of the network on the validation images: 72 %\n",
            "[7,  6400/50000] loss: 0.6228657\n",
            "[7, 12800/50000] loss: 0.5582725\n",
            "[7, 19200/50000] loss: 0.5769671\n",
            "[7, 25600/50000] loss: 0.5908146\n",
            "[7, 32000/50000] loss: 0.5703697\n",
            "[7, 38400/50000] loss: 0.5221429\n",
            "[7, 44800/50000] loss: 0.5632979\n",
            "[7 epoch] Accuracy of the network on the validation images: 74 %\n",
            "[8,  6400/50000] loss: 0.5454543\n",
            "[8, 12800/50000] loss: 0.5230950\n",
            "[8, 19200/50000] loss: 0.5256193\n",
            "[8, 25600/50000] loss: 0.5216694\n",
            "[8, 32000/50000] loss: 0.5218557\n",
            "[8, 38400/50000] loss: 0.5403124\n",
            "[8, 44800/50000] loss: 0.5100285\n",
            "[8 epoch] Accuracy of the network on the validation images: 76 %\n",
            "[9,  6400/50000] loss: 0.5084429\n",
            "[9, 12800/50000] loss: 0.4986375\n",
            "[9, 19200/50000] loss: 0.5020505\n",
            "[9, 25600/50000] loss: 0.4937770\n",
            "[9, 32000/50000] loss: 0.4875810\n",
            "[9, 38400/50000] loss: 0.4924445\n",
            "[9, 44800/50000] loss: 0.4914329\n",
            "[9 epoch] Accuracy of the network on the validation images: 78 %\n",
            "[10,  6400/50000] loss: 0.4855072\n",
            "[10, 12800/50000] loss: 0.4711761\n",
            "[10, 19200/50000] loss: 0.4348083\n",
            "[10, 25600/50000] loss: 0.4635098\n",
            "[10, 32000/50000] loss: 0.4647035\n",
            "[10, 38400/50000] loss: 0.4458188\n",
            "[10, 44800/50000] loss: 0.4502745\n",
            "[10 epoch] Accuracy of the network on the validation images: 78 %\n",
            "[11,  6400/50000] loss: 0.4354466\n",
            "[11, 12800/50000] loss: 0.4303710\n",
            "[11, 19200/50000] loss: 0.4323936\n",
            "[11, 25600/50000] loss: 0.4472776\n",
            "[11, 32000/50000] loss: 0.4309691\n",
            "[11, 38400/50000] loss: 0.4655517\n",
            "[11, 44800/50000] loss: 0.4416987\n",
            "[11 epoch] Accuracy of the network on the validation images: 78 %\n",
            "[12,  6400/50000] loss: 0.4357254\n",
            "[12, 12800/50000] loss: 0.4213831\n",
            "[12, 19200/50000] loss: 0.4086507\n",
            "[12, 25600/50000] loss: 0.4241197\n",
            "[12, 32000/50000] loss: 0.4172756\n",
            "[12, 38400/50000] loss: 0.4011533\n",
            "[12, 44800/50000] loss: 0.4355068\n",
            "[12 epoch] Accuracy of the network on the validation images: 79 %\n",
            "[13,  6400/50000] loss: 0.4088587\n",
            "[13, 12800/50000] loss: 0.3951776\n",
            "[13, 19200/50000] loss: 0.3850757\n",
            "[13, 25600/50000] loss: 0.3736539\n",
            "[13, 32000/50000] loss: 0.3960767\n",
            "[13, 38400/50000] loss: 0.3896674\n",
            "[13, 44800/50000] loss: 0.4088914\n",
            "[13 epoch] Accuracy of the network on the validation images: 81 %\n",
            "[14,  6400/50000] loss: 0.3786042\n",
            "[14, 12800/50000] loss: 0.3651391\n",
            "[14, 19200/50000] loss: 0.3776941\n",
            "[14, 25600/50000] loss: 0.3986661\n",
            "[14, 32000/50000] loss: 0.3824317\n",
            "[14, 38400/50000] loss: 0.3822890\n",
            "[14, 44800/50000] loss: 0.3884934\n",
            "[14 epoch] Accuracy of the network on the validation images: 81 %\n",
            "[15,  6400/50000] loss: 0.3589025\n",
            "[15, 12800/50000] loss: 0.3460606\n",
            "[15, 19200/50000] loss: 0.3433624\n",
            "[15, 25600/50000] loss: 0.3697639\n",
            "[15, 32000/50000] loss: 0.3518920\n",
            "[15, 38400/50000] loss: 0.3684080\n",
            "[15, 44800/50000] loss: 0.3725641\n",
            "[15 epoch] Accuracy of the network on the validation images: 82 %\n",
            "[16,  6400/50000] loss: 0.3288793\n",
            "[16, 12800/50000] loss: 0.3305816\n",
            "[16, 19200/50000] loss: 0.3395676\n",
            "[16, 25600/50000] loss: 0.3655725\n",
            "[16, 32000/50000] loss: 0.3474058\n",
            "[16, 38400/50000] loss: 0.3819626\n",
            "[16, 44800/50000] loss: 0.3741987\n",
            "[16 epoch] Accuracy of the network on the validation images: 83 %\n",
            "[17,  6400/50000] loss: 0.3191682\n",
            "[17, 12800/50000] loss: 0.3216789\n",
            "[17, 19200/50000] loss: 0.3433073\n",
            "[17, 25600/50000] loss: 0.3406458\n",
            "[17, 32000/50000] loss: 0.3186692\n",
            "[17, 38400/50000] loss: 0.3429068\n",
            "[17, 44800/50000] loss: 0.3484635\n",
            "[17 epoch] Accuracy of the network on the validation images: 83 %\n",
            "[18,  6400/50000] loss: 0.3287819\n",
            "[18, 12800/50000] loss: 0.3225906\n",
            "[18, 19200/50000] loss: 0.3224047\n",
            "[18, 25600/50000] loss: 0.3127985\n",
            "[18, 32000/50000] loss: 0.3569726\n",
            "[18, 38400/50000] loss: 0.3138424\n",
            "[18, 44800/50000] loss: 0.3233106\n",
            "[18 epoch] Accuracy of the network on the validation images: 84 %\n",
            "[19,  6400/50000] loss: 0.3105539\n",
            "[19, 12800/50000] loss: 0.3052158\n",
            "[19, 19200/50000] loss: 0.2795094\n",
            "[19, 25600/50000] loss: 0.3065538\n",
            "[19, 32000/50000] loss: 0.3236094\n",
            "[19, 38400/50000] loss: 0.3299916\n",
            "[19, 44800/50000] loss: 0.3001464\n",
            "[19 epoch] Accuracy of the network on the validation images: 84 %\n",
            "[20,  6400/50000] loss: 0.2282974\n",
            "[20, 12800/50000] loss: 0.2241019\n",
            "[20, 19200/50000] loss: 0.2045272\n",
            "[20, 25600/50000] loss: 0.1977383\n",
            "[20, 32000/50000] loss: 0.2069640\n",
            "[20, 38400/50000] loss: 0.1933619\n",
            "[20, 44800/50000] loss: 0.1873313\n",
            "[20 epoch] Accuracy of the network on the validation images: 87 %\n",
            "[21,  6400/50000] loss: 0.1902802\n",
            "[21, 12800/50000] loss: 0.1839743\n",
            "[21, 19200/50000] loss: 0.1880799\n",
            "[21, 25600/50000] loss: 0.1742081\n",
            "[21, 32000/50000] loss: 0.1893250\n",
            "[21, 38400/50000] loss: 0.1683663\n",
            "[21, 44800/50000] loss: 0.1845183\n",
            "[21 epoch] Accuracy of the network on the validation images: 86 %\n",
            "[22,  6400/50000] loss: 0.1820559\n",
            "[22, 12800/50000] loss: 0.1785639\n",
            "[22, 19200/50000] loss: 0.1572830\n",
            "[22, 25600/50000] loss: 0.1799615\n",
            "[22, 32000/50000] loss: 0.1758500\n",
            "[22, 38400/50000] loss: 0.1745236\n",
            "[22, 44800/50000] loss: 0.1686155\n",
            "[22 epoch] Accuracy of the network on the validation images: 86 %\n",
            "[23,  6400/50000] loss: 0.1563950\n",
            "[23, 12800/50000] loss: 0.1608650\n",
            "[23, 19200/50000] loss: 0.1739816\n",
            "[23, 25600/50000] loss: 0.1576475\n",
            "[23, 32000/50000] loss: 0.1747238\n",
            "[23, 38400/50000] loss: 0.1497116\n",
            "[23, 44800/50000] loss: 0.1652668\n",
            "[23 epoch] Accuracy of the network on the validation images: 86 %\n",
            "[24,  6400/50000] loss: 0.1593849\n",
            "[24, 12800/50000] loss: 0.1628408\n",
            "[24, 19200/50000] loss: 0.1441420\n",
            "[24, 25600/50000] loss: 0.1706857\n",
            "[24, 32000/50000] loss: 0.1681341\n",
            "[24, 38400/50000] loss: 0.1579044\n",
            "[24, 44800/50000] loss: 0.1606024\n",
            "[24 epoch] Accuracy of the network on the validation images: 87 %\n",
            "[25,  6400/50000] loss: 0.1537321\n",
            "[25, 12800/50000] loss: 0.1632604\n",
            "[25, 19200/50000] loss: 0.1494564\n",
            "[25, 25600/50000] loss: 0.1508358\n",
            "[25, 32000/50000] loss: 0.1504414\n",
            "[25, 38400/50000] loss: 0.1573982\n",
            "[25, 44800/50000] loss: 0.1516375\n",
            "[25 epoch] Accuracy of the network on the validation images: 86 %\n",
            "[26,  6400/50000] loss: 0.1547324\n",
            "[26, 12800/50000] loss: 0.1557823\n",
            "[26, 19200/50000] loss: 0.1480105\n",
            "[26, 25600/50000] loss: 0.1455500\n",
            "[26, 32000/50000] loss: 0.1395182\n",
            "[26, 38400/50000] loss: 0.1599927\n",
            "[26, 44800/50000] loss: 0.1470167\n",
            "[26 epoch] Accuracy of the network on the validation images: 86 %\n",
            "[27,  6400/50000] loss: 0.1471681\n",
            "[27, 12800/50000] loss: 0.1557681\n",
            "[27, 19200/50000] loss: 0.1519860\n",
            "[27, 25600/50000] loss: 0.1544357\n",
            "[27, 32000/50000] loss: 0.1449228\n",
            "[27, 38400/50000] loss: 0.1322386\n",
            "[27, 44800/50000] loss: 0.1508156\n",
            "[27 epoch] Accuracy of the network on the validation images: 87 %\n",
            "[28,  6400/50000] loss: 0.1355011\n",
            "[28, 12800/50000] loss: 0.1379783\n",
            "[28, 19200/50000] loss: 0.1360825\n",
            "[28, 25600/50000] loss: 0.1478749\n",
            "[28, 32000/50000] loss: 0.1456438\n",
            "[28, 38400/50000] loss: 0.1365022\n",
            "[28, 44800/50000] loss: 0.1558128\n",
            "[28 epoch] Accuracy of the network on the validation images: 86 %\n",
            "[29,  6400/50000] loss: 0.1387826\n",
            "[29, 12800/50000] loss: 0.1345303\n",
            "[29, 19200/50000] loss: 0.1354890\n",
            "[29, 25600/50000] loss: 0.1347013\n",
            "[29, 32000/50000] loss: 0.1410723\n",
            "[29, 38400/50000] loss: 0.1371522\n",
            "[29, 44800/50000] loss: 0.1359737\n",
            "[29 epoch] Accuracy of the network on the validation images: 87 %\n",
            "[30,  6400/50000] loss: 0.1343307\n",
            "[30, 12800/50000] loss: 0.1398514\n",
            "[30, 19200/50000] loss: 0.1216913\n",
            "[30, 25600/50000] loss: 0.1286544\n",
            "[30, 32000/50000] loss: 0.1271037\n",
            "[30, 38400/50000] loss: 0.1237615\n",
            "[30, 44800/50000] loss: 0.1233400\n",
            "[30 epoch] Accuracy of the network on the validation images: 87 %\n",
            "[31,  6400/50000] loss: 0.1255842\n",
            "[31, 12800/50000] loss: 0.1341271\n",
            "[31, 19200/50000] loss: 0.1198322\n",
            "[31, 25600/50000] loss: 0.1359381\n",
            "[31, 32000/50000] loss: 0.1146219\n",
            "[31, 38400/50000] loss: 0.1310069\n",
            "[31, 44800/50000] loss: 0.1286265\n",
            "[31 epoch] Accuracy of the network on the validation images: 87 %\n",
            "[32,  6400/50000] loss: 0.1171930\n",
            "[32, 12800/50000] loss: 0.1401966\n",
            "[32, 19200/50000] loss: 0.1240022\n",
            "[32, 25600/50000] loss: 0.1247085\n",
            "[32, 32000/50000] loss: 0.1210692\n",
            "[32, 38400/50000] loss: 0.1186778\n",
            "[32, 44800/50000] loss: 0.1250817\n",
            "[32 epoch] Accuracy of the network on the validation images: 87 %\n",
            "[33,  6400/50000] loss: 0.1351548\n",
            "[33, 12800/50000] loss: 0.1282170\n",
            "[33, 19200/50000] loss: 0.1317832\n",
            "[33, 25600/50000] loss: 0.1267399\n",
            "[33, 32000/50000] loss: 0.1271454\n",
            "[33, 38400/50000] loss: 0.1096100\n",
            "[33, 44800/50000] loss: 0.1262506\n",
            "[33 epoch] Accuracy of the network on the validation images: 87 %\n",
            "[34,  6400/50000] loss: 0.1251740\n",
            "[34, 12800/50000] loss: 0.1303256\n",
            "[34, 19200/50000] loss: 0.1246160\n",
            "[34, 25600/50000] loss: 0.1199836\n",
            "[34, 32000/50000] loss: 0.1177884\n",
            "[34, 38400/50000] loss: 0.1304588\n",
            "[34, 44800/50000] loss: 0.1319054\n",
            "[34 epoch] Accuracy of the network on the validation images: 87 %\n",
            "[35,  6400/50000] loss: 0.1156964\n",
            "[35, 12800/50000] loss: 0.1110226\n",
            "[35, 19200/50000] loss: 0.1227055\n",
            "[35, 25600/50000] loss: 0.1210041\n",
            "[35, 32000/50000] loss: 0.1186430\n",
            "[35, 38400/50000] loss: 0.1277770\n",
            "[35, 44800/50000] loss: 0.1322040\n",
            "[35 epoch] Accuracy of the network on the validation images: 87 %\n",
            "[36,  6400/50000] loss: 0.1210081\n",
            "[36, 12800/50000] loss: 0.1199751\n",
            "[36, 19200/50000] loss: 0.1210861\n",
            "[36, 25600/50000] loss: 0.1180347\n",
            "[36, 32000/50000] loss: 0.1267594\n",
            "[36, 38400/50000] loss: 0.1186031\n",
            "[36, 44800/50000] loss: 0.1324206\n",
            "[36 epoch] Accuracy of the network on the validation images: 87 %\n",
            "[37,  6400/50000] loss: 0.1197955\n",
            "[37, 12800/50000] loss: 0.1188006\n",
            "[37, 19200/50000] loss: 0.1117347\n",
            "[37, 25600/50000] loss: 0.1214377\n",
            "[37, 32000/50000] loss: 0.1269683\n",
            "[37, 38400/50000] loss: 0.1196071\n",
            "[37, 44800/50000] loss: 0.1304811\n",
            "[37 epoch] Accuracy of the network on the validation images: 87 %\n",
            "[38,  6400/50000] loss: 0.1177637\n",
            "[38, 12800/50000] loss: 0.1259845\n",
            "[38, 19200/50000] loss: 0.1311801\n",
            "[38, 25600/50000] loss: 0.1197133\n",
            "[38, 32000/50000] loss: 0.1209898\n",
            "[38, 38400/50000] loss: 0.1257657\n",
            "[38, 44800/50000] loss: 0.1306718\n",
            "[38 epoch] Accuracy of the network on the validation images: 87 %\n",
            "[39,  6400/50000] loss: 0.1099215\n",
            "[39, 12800/50000] loss: 0.1206015\n",
            "[39, 19200/50000] loss: 0.1232545\n",
            "[39, 25600/50000] loss: 0.1279350\n",
            "[39, 32000/50000] loss: 0.1266736\n",
            "[39, 38400/50000] loss: 0.1203045\n",
            "[39, 44800/50000] loss: 0.1303828\n",
            "[39 epoch] Accuracy of the network on the validation images: 87 %\n",
            "[40,  6400/50000] loss: 0.1163013\n",
            "[40, 12800/50000] loss: 0.1230977\n",
            "[40, 19200/50000] loss: 0.1281872\n",
            "[40, 25600/50000] loss: 0.1182678\n",
            "[40, 32000/50000] loss: 0.1224242\n",
            "[40, 38400/50000] loss: 0.1256918\n",
            "[40, 44800/50000] loss: 0.1162288\n",
            "[40 epoch] Accuracy of the network on the validation images: 87 %\n",
            "Finished Training\n",
            "The total time to train the model on the K80 GPU is : 89.1 minutes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "BgoEx7vz6tOZ",
        "outputId": "417058bc-8cf8-4219-ec1d-b18395b11d22"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x = range(len(loss_vector))\n",
        "x_epoch = [z for z in range(1, len(loss_vector)+1) if z%50 == 0] \n",
        "x_ticks_labels = [str(x) for x in range(1, 40)]\n",
        "\n",
        "plt.figure(1)\n",
        "print(f\"x = {x}\")\n",
        "print(f\"loss_vector = {loss_vector}\")\n",
        "print()\n",
        "plt.plot(x, loss_vector, color = 'b', label = \"Training Loss: DenseNetBC_100_12\")\n",
        "plt.xticks(x_epoch, x_ticks_labels)\n",
        "plt.title(\"Training Error Plot\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.xlabel(\"No. of Epochs                                                                                 x 50\")\n",
        "plt.legend()\n",
        "plt.savefig(\"training_error_plot.png\", dpi = 300) # to save your plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x = range(0, 280)\n",
            "loss_vector = [1.9896437919139862, 1.8472651755809784, 1.742215371131897, 1.627052229642868, 1.5546020400524139, 1.467852166891098, 1.401735965013504, 1.3521482706069947, 1.2921676874160766, 1.2528611940145493, 1.1459972339868545, 1.1028979671001435, 1.0719605374336243, 1.096458038687706, 1.0502052944898606, 0.9985136717557908, 0.9693761670589447, 0.9674620050191879, 0.8693731671571732, 0.8971370634436607, 0.9046278232336045, 0.8780809390544891, 0.8545940124988556, 0.8556323623657227, 0.7780161234736442, 0.7971866172552109, 0.7591564273834228, 0.7330044654011726, 0.7515835383534432, 0.7076417708396912, 0.7071316942572594, 0.6994009691476822, 0.6765867730975151, 0.6906430661678314, 0.6774377086758614, 0.6672927758097649, 0.6543112993240356, 0.6281163328886032, 0.6404517138004303, 0.6142701935768128, 0.614784369468689, 0.591631892323494, 0.6228656607866287, 0.5582725363969803, 0.5769671449065208, 0.5908145615458489, 0.5703697320818901, 0.5221429285407067, 0.5632978937029839, 0.5454542967677116, 0.5230949692428112, 0.5256193090975284, 0.5216693975031376, 0.5218556922674179, 0.5403123554587365, 0.5100285103917122, 0.5084429214894771, 0.4986375388503075, 0.5020505237579346, 0.4937769542634487, 0.48758098810911177, 0.49244447842240335, 0.4914328670501709, 0.48550724163651465, 0.471176068931818, 0.4348082514107227, 0.4635097739100456, 0.4647034579515457, 0.44581878304481504, 0.45027448192238806, 0.43544658571481704, 0.430370973944664, 0.4323935841023922, 0.44727757170796395, 0.4309690563380718, 0.4655516639351845, 0.44169870764017105, 0.43572537884116175, 0.42138306275010107, 0.40865072220563886, 0.4241196586191654, 0.41727556809782984, 0.401153267621994, 0.43550683200359347, 0.40885871574282645, 0.3951775752007961, 0.3850756759941578, 0.3736538702249527, 0.3960766765475273, 0.38966743543744087, 0.40889144510030745, 0.37860417753458026, 0.36513910457491877, 0.3776940885186195, 0.3986660957336426, 0.38243174031376836, 0.38228899776935577, 0.38849337175488474, 0.35890247866511343, 0.34606059819459917, 0.34336236625909805, 0.3697638717293739, 0.35189196810126305, 0.36840796962380407, 0.37256406724452973, 0.32887933507561684, 0.33058162704110144, 0.33956757687032224, 0.3655725149065256, 0.34740579217672346, 0.38196261450648306, 0.3741986906528473, 0.3191682329773903, 0.32167894646525386, 0.3433073087781668, 0.34064575269818304, 0.3186692099273205, 0.34290681794285777, 0.34846353173255923, 0.32878193065524103, 0.32259055465459824, 0.32240469701588154, 0.31279845342040064, 0.35697264775633814, 0.31384237855672836, 0.32331062138080596, 0.31055391266942023, 0.30521580561995504, 0.2795094285905361, 0.3065538238734007, 0.32360936760902403, 0.3299915605783463, 0.3001463831961155, 0.22829739309847355, 0.22410187676548957, 0.2045272124931216, 0.19773834496736525, 0.2069639553129673, 0.19336187351495027, 0.18733127422630788, 0.19028020091354847, 0.1839742859825492, 0.18807992793619632, 0.17420806180685758, 0.18932498205453158, 0.1683662913367152, 0.18451830226927995, 0.18205594655126334, 0.1785638654977083, 0.15728301513940096, 0.1799615316465497, 0.17585001416504384, 0.17452364794909955, 0.1686154566705227, 0.15639504041522742, 0.1608650441095233, 0.17398162573575973, 0.1576475363969803, 0.17472377855330706, 0.14971164703369141, 0.16526681780815125, 0.15938489038497208, 0.16284081157296895, 0.14414201959967612, 0.17068567540496588, 0.1681341265514493, 0.1579043758660555, 0.1606023706868291, 0.15373211033642292, 0.1632604404166341, 0.14945639215409756, 0.1508358352072537, 0.15044135335832834, 0.15739816330373288, 0.1516375320777297, 0.15473239958286286, 0.1557822747901082, 0.14801053507253528, 0.14555000284686684, 0.13951818995177745, 0.15999267369508743, 0.14701668053865433, 0.14716811046004297, 0.1557681291550398, 0.15198601864278316, 0.1544357386045158, 0.14492278721183538, 0.13223861414939164, 0.15081562638282775, 0.1355010983720422, 0.1379782997816801, 0.13608250173740088, 0.14787486758083104, 0.1456438310816884, 0.13650220032781363, 0.15581281980499626, 0.1387825866229832, 0.134530271217227, 0.1354890052229166, 0.13470127023756504, 0.141072293240577, 0.13715221669524907, 0.13597372271120547, 0.13433069408871234, 0.13985142908990383, 0.12169129874557257, 0.12865439482033253, 0.12710370302200316, 0.12376145470887423, 0.12334002509713173, 0.12558420654386282, 0.1341271463409066, 0.11983218798413872, 0.1359381266310811, 0.11462190795689821, 0.13100690422579647, 0.12862648420035838, 0.11719299383461475, 0.14019662395119667, 0.12400217466056347, 0.1247084684856236, 0.12106918239966034, 0.11867780726402997, 0.1250816756300628, 0.1351548435166478, 0.1282170332595706, 0.1317832157574594, 0.1267399137467146, 0.12714541919529437, 0.10960996987298131, 0.1262505617365241, 0.12517399819567798, 0.1303255978040397, 0.12461596589535474, 0.11998357240110635, 0.11778844001702965, 0.13045880999416112, 0.13190538085997106, 0.1156963768415153, 0.11102257814258337, 0.12270550265908241, 0.12100414272397757, 0.11864302046597004, 0.12777704980224372, 0.13220399865880608, 0.12100813375785947, 0.11997514113783836, 0.12108613055199385, 0.11803467319346965, 0.12675937559455633, 0.11860310120508075, 0.13242062967270612, 0.1197954767011106, 0.11880060562863946, 0.11173473302274943, 0.12143767569214106, 0.12696834467351437, 0.11960708441212774, 0.13048111086711286, 0.11776367904618383, 0.12598447173833846, 0.13118007510900498, 0.11971330875530839, 0.12098981799557805, 0.12576572746038436, 0.13067180167883635, 0.10992145234718918, 0.12060154471546411, 0.12325454132631421, 0.12793502477928997, 0.1266736269183457, 0.12030447643250226, 0.13038277396932244, 0.11630129247903824, 0.12309765469282866, 0.1281872270256281, 0.11826777271926403, 0.12242423962801695, 0.1256917841359973, 0.11622879970818759]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVSrYBu7SVqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0fb144-5de7-46f5-8eba-f28cbe01ed78"
      },
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = densenetBC(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "                \n",
        "        for i in range(labels.shape[0]):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "            \n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))            \n",
        "            \n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
        "print()\n",
        "print(\"You have reached the end of Engelbert's implemenetation\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "Accuracy of plane : 87 %\n",
            "Accuracy of   car : 91 %\n",
            "Accuracy of  bird : 84 %\n",
            "Accuracy of   cat : 76 %\n",
            "Accuracy of  deer : 84 %\n",
            "Accuracy of   dog : 81 %\n",
            "Accuracy of  frog : 89 %\n",
            "Accuracy of horse : 90 %\n",
            "Accuracy of  ship : 92 %\n",
            "Accuracy of truck : 90 %\n",
            "\n",
            "You have reached the end of Engelbert's implemenetation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tovw0opbsuD2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}